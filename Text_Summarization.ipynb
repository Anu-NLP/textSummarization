{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHiVWmgvsvjz",
        "outputId": "e2e5b85a-c07d-43da-cfcc-2759a52c7312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tqdm==4.42.1\n",
            "  Using cached tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\n",
            "Installing collected packages: tqdm\n",
            "\u001b[33m  WARNING: The script tqdm is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.20.0 requires tqdm>=4.66.3, but you have tqdm 4.42.1 which is incompatible.\n",
            "dopamine-rl 4.0.9 requires tqdm>=4.64.1, but you have tqdm 4.42.1 which is incompatible.\n",
            "evaluate 0.4.2 requires tqdm>=4.62.1, but you have tqdm 4.42.1 which is incompatible.\n",
            "panel 1.3.8 requires tqdm>=4.48.0, but you have tqdm 4.42.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tqdm-4.42.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /root/.local/lib/python3.10/site-packages (from transformers) (4.42.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/.local/lib/python3.10/site-packages (from requests->transformers) (1.25.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Collecting tqdm>=4.62.1 (from evaluate)\n",
            "  Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/.local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.25.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.42.1\n",
            "    Uninstalling tqdm-4.42.1:\n",
            "      Successfully uninstalled tqdm-4.42.1\n",
            "Successfully installed tqdm-4.66.4\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (2024.6.2)\n",
            "Requirement already satisfied: urllib3==1.25.10 in /root/.local/lib/python3.10/site-packages (1.25.10)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install --user tqdm==4.42.1\n",
        "!pip install torch\n",
        "!pip install transformers --user\n",
        "!pip install evaluate\n",
        "!pip install --user --upgrade certifi\n",
        "!pip install --user --upgrade urllib3==1.25.10\n",
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge-score py7zr -q\n",
        "!pip install textblob\n",
        "import pandas as pd\n",
        "import urllib3\n",
        "import torch                                                              # PyTorch\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Other NLP libraries\n",
        "from textblob import TextBlob                                             # This is going to help us fix spelling mistakes in texts\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer               # This is going to helps identify the most common terms in the corpus\n",
        "import re                                                                 # This library allows us to clean text data\n",
        "import nltk                                                               # Natural Language Toolkit\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "def generate_summary(text):\n",
        "    # Tokenize input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "    # Generate summary\n",
        "    # Increased max_length and added max_new_tokens\n",
        "    summary_ids = model.generate(input_ids,\n",
        "                                 max_length=500, # Increased to accommodate input and output\n",
        "                                 max_new_tokens=150, # Limit new tokens generated\n",
        "                                 num_beams=2,\n",
        "                                 no_repeat_ngram_size=2,\n",
        "                                 early_stopping=True)\n",
        "\n",
        "    # Decode and return the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Example usage\n",
        "text = '''\n",
        "We will be with you soon.\n",
        "System\n",
        "1:14:11 PM\n",
        "You are currently 1 in queue.\n",
        "System\n",
        "1:14:41 PM\n",
        "INC34955425\n",
        "MICHAEL BORCHARDT\n",
        "1:14:43 PM\n",
        "You are currently 1 in queue.\n",
        "System\n",
        "1:15:41 PM\n",
        "INC34955425\n",
        "MICHAEL BORCHARDT\n",
        "1:16:07 PM\n",
        "Good morning\n",
        "Eddens,Devlin (COACH)\n",
        "1:16:31 PM\n",
        "kindly give me a minute to review the ticket\n",
        "Eddens,Devlin (COACH)\n",
        "1:16:40 PM\n",
        "Metered error in Outlook?\n",
        "Eddens,Devlin (COACH)\n",
        "1:19:15 PM\n",
        "see screen shot. it blips on and off where you cant read it (its the yellow error) and i was able to snap a shot right when it appeared before disappearing again over and over\n",
        "MICHAEL BORCHARDT\n",
        "1:20:05 PM\n",
        "can you add me into bomgar?\n",
        "Eddens,Devlin (COACH)\n",
        "1:20:36 PM\n",
        "yes\n",
        "MICHAEL BORCHARDT\n",
        "1:20:41 PM\n",
        "ty\n",
        "Eddens,Devlin (COACH)\n",
        "1:20:49 PM\n",
        "she should be good to go now, we turned off the metered connection setting and I sent her a successful test email\n",
        "Eddens,Devlin (COACH)\n",
        "1:25:46 PM\n",
        "thanks a lot Devlin. you are awesome\n",
        "MICHAEL BORCHARDT\n",
        "1:26:13 PM\n",
        "what would i use for a KB?\n",
        "MICHAEL BORCHARDT\n",
        "1:26:45 PM\n",
        "My pleasure you are very welcome!! KBB0045163 and Microsoft Outlook - Unable to Connect or Connection is UnavailableKB0104935\n",
        "Eddens,Devlin (COACH)\n",
        "1:26:50 PM\n",
        "thanks\n",
        "MICHAEL BORCHARDT\n",
        "1:27:00 PM\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "summary = generate_summary(text)\n",
        "print(\"summary\" , summary)"
      ],
      "metadata": {
        "id": "HlGs9IJQHpst",
        "outputId": "5f83ee3d-1f3e-41d7-87e6-5f0029bb8106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summary   \n",
            "We will be with you soon.\n",
            "System\n",
            "1:14:11 PM\n",
            "You are currently 1 in queue.\n",
            "System\n",
            "1:14:41 PM\n",
            "INC34955425\n",
            "MICHAEL BORCHARDT\n",
            "1:14:43 PM\n",
            "You are currently 1 in queue.\n",
            "System\n",
            "1:15:41 PM\n",
            "INC34955425\n",
            "MICHAEL BORCHARDT\n",
            "1:16:07 PM\n",
            "Good morning\n",
            "Eddens,Devlin (COACH)\n",
            "1:16:31 PM\n",
            "kindly give me a minute to review the ticket\n",
            "Eddens,Devlin (COACH)\n",
            "1:16:40 PM\n",
            "Metered error in Outlook?\n",
            "Eddens,Devlin (COACH)\n",
            "1:19:15 PM\n",
            "see screen shot. it blips on and off where you cant read it (its the yellow error) and i was able to snap a shot right when it appeared before disappearing again over and over\n",
            "MICHAEL BORCHARDT\n",
            "1:20:05 PM\n",
            "can you add me into bomgar?\n",
            "Eddens,Devlin (COACH)\n",
            "1:20:36 PM\n",
            "yes\n",
            "MICHAEL BORCHARDT\n",
            "1:20:41 PM\n",
            "ty\n",
            "Eddens,Devlin (COACH)\n",
            "1:20:49 PM\n",
            "she should be good to go now, we turned off the metered connection setting and I sent her a successful test email\n",
            "Eddens,Devlin (COACH)\n",
            "1:25:46 PM\n",
            "thanks a lot Devlin. you are awesome\n",
            "MICHAEL BORCHARDT\n",
            "1:26:13 PM\n",
            "what would i use for a KB?\n",
            "MICHAEL BORCHARDT\n",
            "1:26:45 PM\n",
            "My pleasure you are very welcome!! KBB0045163 and Microsoft Outlook - Unable to Connect or Connection is UnavailableKB0104935\n",
            "Eddens,Devlin (COACH)\n",
            "1:26:50 PM\n",
            "thanks\n",
            "MICHAEL BORCHARDT\n",
            "1:27:00 PM\n",
            "\n",
            "  \n",
            "Thanks for your time, I hope you have a great day. __________________\n",
            "2:02:30 PM __________ __________________________\n",
            "\n",
            "\n",
            "3:03:10 PM I'm sorry for the delay, but I can't get the email to work. I'll try to get it back to you as soon as possible. Thanks again for all your help.\n",
            "\n",
            " (I'm not sure if it's because of the time difference or because I forgot to send it to my email address.) _______________________________________________\n",
            "4:04:17 PM (1) _________________________________________________________________________\n",
            "\n",
            "5:06:18 PM You are now 2nd in line for this ticket. You will need to fill out the form below to enter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from google.colab import userdata\n",
        "userdata.get('Gp2-textsummarization')\n",
        "model_name = 'gpt2-medium'  # Change to 'gpt2-medium' or 'gpt2-large' for more powerful models\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set the pad token ID to the EOS token ID\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Encode the input text\n",
        "input_ids = tokenizer.encode(conversation, return_tensors=\"pt\")\n",
        "\n",
        "# Generate the output\n",
        "output = model.generate(input_ids, attention_mask=input_ids)\n",
        "\n",
        "# Decode the output\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the output text\n",
        "print(output_text)\n",
        "\n",
        "\n",
        "\n",
        "# Example conversation\n",
        "conversation = '''\n",
        "User: Can you help me find some good restaurants in my area?\n",
        "Assistant: Sure! I recommend using platforms like Yelp or TripAdvisor to find popular restaurants with good reviews.\n",
        "User: Thanks! I'll check them out.\n",
        "Assistant: You're welcome! Is there anything else I can assist you with?\n",
        "User: Yes, I need some movie suggestions. Any recommendations?\n",
        "Assistant: Of course! What genre do you prefer?\n",
        "User: I like action movies.\n",
        "Assistant: Great! Some recent action movies you might enjoy are \"Avengers: Endgame\" and \"Mission: Impossible - Fallout\".\n",
        "User: Thanks for the suggestions!\n",
        "Assistant: You're welcome! Let me know if there's anything else I can help with.\n",
        "User: Actually, I'm planning a trip to Europe. Any travel tips?\n",
        "Assistant: Absolutely! Make sure to pack light, research local customs, and consider using public transportation to get around.\n",
        "User: Good advice! What are some popular tourist destinations in Europe?\n",
        "Assistant: Cities like Paris, Rome, and Barcelona are quite popular due to their historical significance and attractions.\n",
        "User: Thanks for the tips!\n",
        "Assistant: You're welcome! Enjoy your trip!\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "eutTFRmVLtzL",
        "outputId": "a71f3a57-56e1-4f60-8398-37ee1c2e8bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410,
          "referenced_widgets": [
            "a558800867ad47d6851b1972c825883c",
            "d19609114ebb4c64b3e1ec27274a4d8b",
            "b31f025eb5494fab9d3addbd13673abc",
            "1491e4fa3a764153ac74f995cc91b453",
            "40dcb43017a04032b0af4d0f9915f276",
            "f3bb8b3c7b624eaca63514ba3d5e03ac",
            "f145a15178ee4ae98e0f9b2cf2f60dc0",
            "769dc49a658144ac87ad7dd11d9d15b7",
            "effee6263536468d92c040734fdc8e3a",
            "1a260d0a5b9048bb87357fe52293cedd",
            "4f80fa67c55c4fe9ac5c7253673230ee",
            "210c7e8dba7c4612bfe4dfc04bd4ae11",
            "4ee1dfe01271421d9c7257974a88c67b",
            "cf380a3103be4bd7a82cf8209cca95e2",
            "cf2f076543434729b4eaccaa79e30e14",
            "90e6c8c976f54b8ebaee1ce25ff589cb",
            "96cd9a1f30a54b14851cee8c7d91abaa",
            "bec0fe2a41954ed39de08f86633aabc1",
            "7c43d11c21c141fb9b0fe55ba17e4b41",
            "ae7e1f30caef4a1d97f4018f9951f0ea",
            "882c1f787aac4ead9afebc6e0077c2fd",
            "e6de74ad3a4c481e944c111a77d807af"
          ]
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a558800867ad47d6851b1972c825883c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "210c7e8dba7c4612bfe4dfc04bd4ae11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GPT2LMHeadModel' object has no attribute 'eos_token'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b59ae557fb6e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Set the pad token ID to the EOS token ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GPT2LMHeadModel' object has no attribute 'eos_token'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
        "# Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. how to do that ?\n",
        "\n",
        "# Create a GPT2Tokenizer instance\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set the pad token ID to the EOS token ID\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Encode the input text\n",
        "input_ids = tokenizer.encode(\"Hello, world!\", return_tensors=\"pt\")\n",
        "\n",
        "# Generate the output\n",
        "output = model.generate(input_ids, attention_mask=input_ids)\n",
        "\n",
        "# Decode the output\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the output text\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "id": "FhTtKs4hikyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "def generate_summary(text):\n",
        "    model_name = \"llama\"  # Llama model\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "    # Use LlamaForCausalLM instead of LlamaForConditionalGeneration\n",
        "    model = LlamaForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    summary_ids = model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Example text\n",
        "text = \"\"\"\n",
        "    Transformers is a deep learning model architecture that has revolutionized the field of natural language processing. It has been widely used for tasks like text classification, machine translation, and text generation. The Llama model is a variant of Transformers specifically designed for text summarization. It excels at generating concise summaries of long documents or articles. Llama leverages the power of self-attention mechanisms to capture important information and generate coherent summaries. It has achieved state-of-the-art results on various benchmark datasets for text summarization. In this example, we will use Llama to generate a summary of a given text.\n",
        "    \"\"\"\n",
        "\n",
        "summary = generate_summary(text)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "vPoV2sfiMkeB",
        "outputId": "a546b3a2-a182-4335-ef71-325ed1def924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed transformers-4.42.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "706cba135a814926a0e9d720c2104e3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "llama is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/llama/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0muser_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_user_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1222\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hf_file_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEntryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1646\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    351\u001b[0m             )\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-668581ca-116b1a8b3a5685ac1fcc8e63;d508da4e-c23b-4fa1-afb2-520437e41be9)\n\nRepository Not Found for url: https://huggingface.co/llama/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d98e1a5885e2>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"  \n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d98e1a5885e2>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"llama\"\u001b[0m  \u001b[0;31m# Llama model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Use LlamaForCausalLM instead of LlamaForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2027\u001b[0m             warnings.warn(\n\u001b[1;32m   2028\u001b[0m                 \u001b[0;34m\"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2029\u001b[0;31m                 \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2030\u001b[0m             )\n\u001b[1;32m   2031\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m         raise EnvironmentError(\n\u001b[1;32m    421\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         ) from e\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: llama is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Rqyr1QzRF5"
      },
      "outputs": [],
      "source": [
        "# Data Handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "# Data Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import plotly.subplots as sp\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.io as pio\n",
        "from IPython.display import display\n",
        "from plotly.offline import init_notebook_mode\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "# Statistics & Mathematics\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import shapiro, skew, anderson, kstest, gaussian_kde,spearmanr\n",
        "import math\n",
        "\n",
        "# Hiding warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuF4LLfJHjvM"
      },
      "outputs": [],
      "source": [
        "jupyter labextension list\n",
        "pip uninstall jupyterlab_widgets\n",
        "pip install jupyterlab_widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMcwtANPtT41"
      },
      "outputs": [],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Load the pre-trained BART model and tokenizer\n",
        "model_name = 'facebook/bart-large-cnn'\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def summarize_chat(chat_text):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer.encode(\"summarize: \" + chat_text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate the summary\n",
        "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Example chat conversation\n",
        "chat_conversation = \"\"\"\n",
        "User1: Hey, how are you doing today?\n",
        "User2: I'm doing well, thanks! How about you?\n",
        "User1: I'm good too. Just a bit tired from work.\n",
        "User2: I understand. Work has been hectic for me as well.\n",
        "User1: What have you been working on lately?\n",
        "User2: Mainly some new projects and reports. It's been a busy week.\n",
        "User1: Sounds like it! Let's catch up more this weekend.\n",
        "User2: Definitely. Looking forward to it!\n",
        "\"\"\"\n",
        "\n",
        "# Summarize the chat conversation\n",
        "summary = summarize_chat(chat_conversation)\n",
        "print(\"Summary:\", summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNgRw4kPtvMK"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the pre-trained T5 model and tokenizer\n",
        "model_name = 't5-small'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def summarize_chat(chat_text):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer.encode(\"summarize: \" + chat_text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate the summary\n",
        "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Example chat conversation\n",
        "chat_conversation = \"\"\"\n",
        "User1: Hey, how are you doing today?\n",
        "User2: I'm doing well, thanks! How about you?\n",
        "User1: I'm good too. Just a bit tired from work.\n",
        "User2: I understand. Work has been hectic for me as well.\n",
        "User1: What have you been working on lately?\n",
        "User2: Mainly some new projects and reports. It's been a busy week.\n",
        "User1: Sounds like it! Let's catch up more this weekend.\n",
        "User2: Definitely. Looking forward to it!\n",
        "\"\"\"\n",
        "\n",
        "# Summarize the chat conversation\n",
        "summary = summarize_chat(chat_conversation)\n",
        "print(\"Summary:\", summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A1_mYCq72s5"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration      # BERT Tokenizer and architecture\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments         # These will help us to fine-tune our model\n",
        "from transformers import pipeline                                         # Pipeline\n",
        "from transformers import DataCollatorForSeq2Seq                           # DataCollator to batch the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-14Krso06WC"
      },
      "outputs": [],
      "source": [
        "checkpoint = 'facebook/bart-large-xsum' # Model\n",
        "tokenizer = BartTokenizer.from_pretrained(checkpoint) # Loading Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLWfj48CvsFV"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "# Load the pre-trained Pegasus model and tokenizer\n",
        "model_name = 'google/pegasus-xsum'\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def summarize_chat(chat_text):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer.encode(chat_text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate the summary\n",
        "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Example chat conversation\n",
        "chat_conversation = \"\"\"\n",
        "User1: Hey, how are you doing today?\n",
        "User2: I'm doing well, thanks! How about you?\n",
        "User1: I'm good too. Just a bit tired from work.\n",
        "User2: I understand. Work has been hectic for me as well.\n",
        "User1: What have you been working on lately?\n",
        "User2: Mainly some new projects and reports. It's been a busy week.\n",
        "User1: Sounds like it! Let's catch up more this weekend.\n",
        "User2: Definitely. Looking forward to it!\n",
        "\"\"\"\n",
        "\n",
        "# Summarize the chat conversation\n",
        "summary = summarize_chat(chat_conversation)\n",
        "print(\"Summary:\", summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7PAolZH1BJE"
      },
      "outputs": [],
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(checkpoint) # Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R8-PRPl1JGG"
      },
      "outputs": [],
      "source": [
        "print(model) # Visualizing model's architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3PBqqxu1Qbv"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [doc for doc in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LNMxnmd1RYX"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lucZNsKE1k07"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric('rouge') # Loading ROUGE Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6TYeIhY1lnt"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred# Obtaining predictions and true labels\n",
        "\n",
        "    # Decoding predictions\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Obtaining the true labels tokens, while eliminating any possible masked token (i.e., label = -100)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "\n",
        "    # Computing rouge score\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()} # Extracting some results\n",
        "\n",
        "    # Add mean-generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeECrTpf1uvl"
      },
      "outputs": [],
      "source": [
        "! pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqbf12no13ZM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "import transformers\n",
        "\n",
        "print(transformers.__version__)\n",
        "\n",
        "#print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UlIWY6f1680"
      },
      "outputs": [],
      "source": [
        "# Defining parameters for training\n",
        "\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install --upgrade accelerate\n",
        "\n",
        "from transformers import Seq2SeqTrainingArguments # Import Seq2SeqTrainingArguments\n",
        "\n",
        "# Defining parameters for training\n",
        "seed = 42 # Set a seed for reproducibility\n",
        "\n",
        "!pip install transformers[torch]\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir = 'bart_samsum',\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = 'eval_loss',\n",
        "    seed = seed,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=True,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE3EJvfz-Egl"
      },
      "outputs": [],
      "source": [
        "def describe_df(df):\n",
        "  \"\"\"Prints a summary of the DataFrame.\"\"\"\n",
        "  print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJOqjeNH6wWc"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pyarrow\n",
        "import pandas as pd\n",
        "!pip install datasets\n",
        "from datasets import Dataset\n",
        "# Configuring Pandas to exhibit larger columns\n",
        "'''\n",
        "This is going to allow us to fully read the dialogues and their summary\n",
        "'''\n",
        "pd.set_option('display.max_colwidth', 1000)\n",
        "train = pd.read_csv('sample_data/samsum-train.csv', encoding='latin-1') # Removed 'content/' from the file path\n",
        "val = pd.read_csv('sample_data/samsum-validation.csv', encoding='latin-1')\n",
        "test = pd.read_csv('sample_data/samsum-test.csv', encoding='latin-1')\n",
        "describe_df(train)\n",
        "# Assuming 'describe_df' is defined elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfuwGXv56Cea"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [doc for doc in examples[\"dialogue\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QADYA2IhEyLI"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "# Transforming dataframes into datasets\n",
        "train_ds = Dataset.from_pandas(train)\n",
        "test_ds = Dataset.from_pandas(test)\n",
        "val_ds = Dataset.from_pandas(val)\n",
        "\n",
        "# Visualizing results\n",
        "print(train_ds)\n",
        "print('\\n' * 2)\n",
        "print(test_ds)\n",
        "print('\\n' * 2)\n",
        "print(val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKgodXMb5y3G"
      },
      "outputs": [],
      "source": [
        "# Applying preprocess_function to the datasets\n",
        "train_ds = train_ds.map(preprocess_function, batched=True,\n",
        "                               remove_columns=['id', 'dialogue', 'Summary Text']) # Corrected column name\n",
        "\n",
        "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "tokenized_train = train_ds.map(preprocess_function, batched=True,\n",
        "                               remove_columns=['id', 'dialogue', 'Summary Text']) # Corrected column name\n",
        "\n",
        "tokenized_test = test_ds.map(preprocess_function, batched=True,\n",
        "                               remove_columns=['id', 'dialogue', 'Summary Text']) # Corrected column name\n",
        "\n",
        "tokenized_val = val_ds.map(preprocess_function, batched=True,\n",
        "                               remove_columns=['id', 'dialogue', 'Summary Text']) # Corrected column name\n",
        "\n",
        "# Printing results\n",
        "print('\\n' * 3)\n",
        "print('Preprocessed Training Dataset:\\n')\n",
        "print(tokenized_train)\n",
        "print('\\n' * 2)\n",
        "print('Preprocessed Test Dataset:\\n')\n",
        "print(tokenized_test)\n",
        "print('\\n' * 2)\n",
        "print('Preprocessed Validation Dataset:\\n')\n",
        "print(tokenized_val)\n",
        "# Printing results\n",
        "print('\\n' * 3)\n",
        "print('Preprocessed Training Dataset:\\n')\n",
        "print(tokenized_train)\n",
        "print('\\n' * 2)\n",
        "print('Preprocessed Test Dataset:\\n')\n",
        "print(tokenized_test)\n",
        "print('\\n' * 2)\n",
        "print('Preprocessed Validation Dataset:\\n')\n",
        "print(tokenized_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H6JV1CI1_FW"
      },
      "outputs": [],
      "source": [
        "# Defining Trainer\n",
        "# Assuming 'tokenized_datasets' is a dictionary containing train and test datasets\n",
        "tokenized_train = tokenized_datasets['train']\n",
        "tokenized_test = tokenized_datasets['test']\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train, # Now this variable is defined\n",
        "    eval_dataset=tokenized_test, # Now this variable is defined\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOe7KHz92G3R"
      },
      "outputs": [],
      "source": [
        "trainer.train() # Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylJXH7BQ2P1q"
      },
      "outputs": [],
      "source": [
        "# Evaluating model performance on the tokenized validation dataset\n",
        "validation = trainer.evaluate(eval_dataset = tokenized_val)\n",
        "print(validation) # Printing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-9Z-D_q2Qz8"
      },
      "outputs": [],
      "source": [
        "# Saving model to a custom directory\n",
        "directory = \"BART_FINETUNED_TEXT_SUMMARY\"\n",
        "trainer.save_model(directory)\n",
        "\n",
        "# Saving model tokenizer\n",
        "tokenizer.save_pretrained(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFYani762XrR"
      },
      "outputs": [],
      "source": [
        "# Saving model in .zip format\n",
        "shutil.make_archive('BART_FINETUNED_TEXT_SUMMARY', 'zip', '/content/BART_FINETUNED_TEXT_SUMMARY')\n",
        "shutil.move('BART_FINETUNED_TEXT_SUMMARY.zip', '/content/BART_FINETUNED_TEXT_SUMMARY.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylZ7hmfu8j__"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-cloud-aiplatform # Make sure you have the latest version of Vertex AI installed\n",
        "\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "# **Provide your Google Cloud Project ID here**\n",
        "PROJECT_ID = \"your-project-id\"\n",
        "!gcloud auth application-default login\n",
        "# Define the model parameters\n",
        "model_name = 'text-bison-001'\n",
        "model_display_name = 'text-bison-001'\n",
        "endpoint_display_name = 'text-bison-001-endpoint'\n",
        "\n",
        "# **Initialize Vertex AI with your project ID**\n",
        "vertexai.init(project=PROJECT_ID)\n",
        "\n",
        "# Create the model resource\n",
        "model = TextGenerationModel.from_pretrained(model_name)\n",
        "model.display_name = model_display_name\n",
        "model.create()\n",
        "\n",
        "# ... (rest of the code remains the same)\n",
        "# Get the endpoint resource\n",
        "endpoint = vertexai.Endpoint(endpoint_name=endpoint_display_name)\n",
        "\n",
        "# Deploy the model to the endpoint\n",
        "endpoint.deploy(model=model, deployed_model_display_name='text-bison-001-deployed')\n",
        "\n",
        "# Wait for the endpoint to be deployed\n",
        "endpoint.wait()\n",
        "\n",
        "# Get the prediction service client\n",
        "prediction_client = vertexai.PredictionServiceClient()\n",
        "\n",
        "# Define the text to be summarized\n",
        "text = \"\"\"\n",
        "User1: Hey, how are you doing today?\n",
        "User2: I'm doing well, thanks! How about you?\n",
        "User1: I'm good too. Just a bit tired from work.\n",
        "User2: I understand. Work has been hectic for me as well.\n",
        "User1: What have you been working on lately?\n",
        "User2: Mainly some new projects and reports. It's been a busy week.\n",
        "User1: Sounds like it! Let's catch up more this weekend.\n",
        "User2: Definitely. Looking forward to it!\n",
        "\"\"\"\n",
        "\n",
        "# Prepare the input for the prediction request\n",
        "payload = {'text': text}\n",
        "\n",
        "# Send the prediction request\n",
        "response = prediction_client.predict(endpoint=endpoint.resource_name, instances=[payload])\n",
        "\n",
        "# Get the summary from the response\n",
        "summary = response.predictions[0]['summary']\n",
        "\n",
        "# Print the summary\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zin7rtz7Gy-f"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "# TODO(developer): Update and un-comment below line\n",
        "project_id = \"PROJECT_ID\" # Remove the extra space here\n",
        "\n",
        "vertexai.init(project=project_id, location=\"us-central1\")\n",
        "\n",
        "# ... (rest of the code remains the same)\n",
        "\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 40,\n",
        "}\n",
        "\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@002\")\n",
        "response = model.predict(\n",
        "    \"\"\"Provide a summary with about two sentences for the following article:\n",
        "    The efficient-market hypothesis (EMH) is a hypothesis in financial \\\n",
        "    economics that states that asset prices reflect all available \\\n",
        "    information. A direct implication is that it is impossible to \\\n",
        "    \"beat the market\" consistently on a risk-adjusted basis since market \\\n",
        "    prices should only react to new information. Because the EMH is \\\n",
        "    formulated in terms of risk adjustment, it only makes testable \\\n",
        "    predictions when coupled with a particular model of risk. As a \\\n",
        "    result, research in financial economics since at least the 1990s has \\\n",
        "    focused on market anomalies, that is, deviations from specific \\\n",
        "    models of risk. The idea that financial market returns are difficult \\\n",
        "    to predict goes back to Bachelier, Mandelbrot, and Samuelson, but \\\n",
        "    is closely associated with Eugene Fama, in part due to his \\\n",
        "    influential 1970 review of the theoretical and empirical research. \\\n",
        "    The EMH provides the basic logic for modern risk-based theories of \\\n",
        "    asset prices, and frameworks such as consumption-based asset pricing \\\n",
        "    and intermediary asset pricing can be thought of as the combination \\\n",
        "    of a model of risk with the EMH. Many decades of empirical research \\\n",
        "    on return predictability has found mixed evidence. Research in the \\\n",
        "    1950s and 1960s often found a lack of predictability (e.g. Ball and \\\n",
        "    Brown 1968; Fama, Fisher, Jensen, and Roll 1969), yet the \\\n",
        "    1980s-2000s saw an explosion of discovered return predictors (e.g. \\\n",
        "    Rosenberg, Reid, and Lanstein 1985; Campbell and Shiller 1988; \\\n",
        "    Jegadeesh and Titman 1993). Since the 2010s, studies have often \\\n",
        "    found that return predictability has become more elusive, as \\\n",
        "    predictability fails to work out-of-sample (Goyal and Welch 2008), \\\n",
        "    or has been weakened by advances in trading technology and investor \\\n",
        "    learning (Chordia, Subrahmanyam, and Tong 2014; McLean and Pontiff \\\n",
        "    2016; Martineau 2021).\n",
        "    Summary:\"\"\",\n",
        "    **parameters,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcQE83xmGGhA"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "!gcloud auth application-default login\n",
        "!gcloud auth application-default set-quota-project project_id\n",
        "# TODO(developer): Update and un-comment below line\n",
        "  project_id = \"113\"\n",
        "\n",
        "vertexai.init(project=project_id, location=\"us-central1\")\n",
        "\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 40,\n",
        "}\n",
        "\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@002\")\n",
        "response = model.predict(\n",
        "    \"\"\"Provide a summary with about two sentences for the following article:\n",
        "    The efficient-market hypothesis (EMH) is a hypothesis in financial \\\n",
        "    economics that states that asset prices reflect all available \\\n",
        "    information. A direct implication is that it is impossible to \\\n",
        "    \"beat the market\" consistently on a risk-adjusted basis since market \\\n",
        "    prices should only react to new information. Because the EMH is \\\n",
        "    formulated in terms of risk adjustment, it only makes testable \\\n",
        "    predictions when coupled with a particular model of risk. As a \\\n",
        "    result, research in financial economics since at least the 1990s has \\\n",
        "    focused on market anomalies, that is, deviations from specific \\\n",
        "    models of risk. The idea that financial market returns are difficult \\\n",
        "    to predict goes back to Bachelier, Mandelbrot, and Samuelson, but \\\n",
        "    is closely associated with Eugene Fama, in part due to his \\\n",
        "    influential 1970 review of the theoretical and empirical research. \\\n",
        "    The EMH provides the basic logic for modern risk-based theories of \\\n",
        "    asset prices, and frameworks such as consumption-based asset pricing \\\n",
        "    and intermediary asset pricing can be thought of as the combination \\\n",
        "    of a model of risk with the EMH. Many decades of empirical research \\\n",
        "    on return predictability has found mixed evidence. Research in the \\\n",
        "    1950s and 1960s often found a lack of predictability (e.g. Ball and \\\n",
        "    Brown 1968; Fama, Fisher, Jensen, and Roll 1969), yet the \\\n",
        "    1980s-2000s saw an explosion of discovered return predictors (e.g. \\\n",
        "    Rosenberg, Reid, and Lanstein 1985; Campbell and Shiller 1988; \\\n",
        "    Jegadeesh and Titman 1993). Since the 2010s, studies have often \\\n",
        "    found that return predictability has become more elusive, as \\\n",
        "    predictability fails to work out-of-sample (Goyal and Welch 2008), \\\n",
        "    or has been weakened by advances in trading technology and investor \\\n",
        "    learning (Chordia, Subrahmanyam, and Tong 2014; McLean and Pontiff \\\n",
        "    2016; Martineau 2021).\n",
        "    Summary:\"\"\",\n",
        "    **parameters,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sz12RWA2kO-"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "710aixnNTdsZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a558800867ad47d6851b1972c825883c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d19609114ebb4c64b3e1ec27274a4d8b",
              "IPY_MODEL_b31f025eb5494fab9d3addbd13673abc",
              "IPY_MODEL_1491e4fa3a764153ac74f995cc91b453"
            ],
            "layout": "IPY_MODEL_40dcb43017a04032b0af4d0f9915f276"
          }
        },
        "d19609114ebb4c64b3e1ec27274a4d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3bb8b3c7b624eaca63514ba3d5e03ac",
            "placeholder": "​",
            "style": "IPY_MODEL_f145a15178ee4ae98e0f9b2cf2f60dc0",
            "value": "model.safetensors: 100%"
          }
        },
        "b31f025eb5494fab9d3addbd13673abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_769dc49a658144ac87ad7dd11d9d15b7",
            "max": 1519984962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_effee6263536468d92c040734fdc8e3a",
            "value": 1519984962
          }
        },
        "1491e4fa3a764153ac74f995cc91b453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a260d0a5b9048bb87357fe52293cedd",
            "placeholder": "​",
            "style": "IPY_MODEL_4f80fa67c55c4fe9ac5c7253673230ee",
            "value": " 1.52G/1.52G [00:09&lt;00:00, 251MB/s]"
          }
        },
        "40dcb43017a04032b0af4d0f9915f276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3bb8b3c7b624eaca63514ba3d5e03ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f145a15178ee4ae98e0f9b2cf2f60dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "769dc49a658144ac87ad7dd11d9d15b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "effee6263536468d92c040734fdc8e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a260d0a5b9048bb87357fe52293cedd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f80fa67c55c4fe9ac5c7253673230ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "210c7e8dba7c4612bfe4dfc04bd4ae11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ee1dfe01271421d9c7257974a88c67b",
              "IPY_MODEL_cf380a3103be4bd7a82cf8209cca95e2",
              "IPY_MODEL_cf2f076543434729b4eaccaa79e30e14"
            ],
            "layout": "IPY_MODEL_90e6c8c976f54b8ebaee1ce25ff589cb"
          }
        },
        "4ee1dfe01271421d9c7257974a88c67b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96cd9a1f30a54b14851cee8c7d91abaa",
            "placeholder": "​",
            "style": "IPY_MODEL_bec0fe2a41954ed39de08f86633aabc1",
            "value": "generation_config.json: 100%"
          }
        },
        "cf380a3103be4bd7a82cf8209cca95e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c43d11c21c141fb9b0fe55ba17e4b41",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae7e1f30caef4a1d97f4018f9951f0ea",
            "value": 124
          }
        },
        "cf2f076543434729b4eaccaa79e30e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_882c1f787aac4ead9afebc6e0077c2fd",
            "placeholder": "​",
            "style": "IPY_MODEL_e6de74ad3a4c481e944c111a77d807af",
            "value": " 124/124 [00:00&lt;00:00, 7.58kB/s]"
          }
        },
        "90e6c8c976f54b8ebaee1ce25ff589cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96cd9a1f30a54b14851cee8c7d91abaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bec0fe2a41954ed39de08f86633aabc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c43d11c21c141fb9b0fe55ba17e4b41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae7e1f30caef4a1d97f4018f9951f0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "882c1f787aac4ead9afebc6e0077c2fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6de74ad3a4c481e944c111a77d807af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}